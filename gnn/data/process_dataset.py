import subprocess
import argparse
import shutil
import json
from os import environ
from pathlib import Path
from typing import Dict

from gnn.data.graph import build_graph
from gnn.data.utils.parsers import extract_metrics


try:
    DSE_LIB = environ['DSE_LIB']
    OPT = environ['OPT']
    CLANG = environ['CLANG']
    LLVM_LINK = environ['LLVM_LINK']
except KeyError as error:
    print(f"Error: environment variable {error.args[0]} not defined.")
    raise


def run_opt(src_path: Path, dst_path: Path, opt_args: str, output_ll=True):
    try:
        if output_ll:
            opt_args += " -S"
        subprocess.check_output(
            f"{OPT} -load {DSE_LIB} {opt_args} < {src_path.as_posix()} > {dst_path.as_posix()};", 
            shell=True, stderr=subprocess.STDOUT
        )
    except subprocess.CalledProcessError as e:
        print(f"Error processing {src_path}: {e}")
        dst_path.unlink(missing_ok=True)
        raise e


def process_ir(src_path: Path, dst_path: Path, metadata_path: Path):
    """
    Process an LLVM IR generated by Vitis HLS for further analysis.

    The following passes are applied to the IR:

    indirectbr-expand: turn indirectbr into switch instructions;
    lowerinvoke: lower invoke instructions to call instructions;
    lowerswitch: lower switch instructions to branch instructions;
    mem2reg: promote memory to register;
    indvars: canonicalize induction variables;
    loop-simplify: simplify the loop structure;
    scalar-evolution: analyze the scalar evolution of the program;
    clear-intrinsics: remove all intrinsic functions;
    assign-ids: assign unique IDs to all instructions, global values, functions and loops.
    extract-md: extract metadata from the IR and write it to a file.

    Args:
        src_path (Path): Path to the original IR file
        dst_path (Path): Path to the file where the processed IR should be written
        metadata_path (Path): Path to the file where the IR metadata should be written
    """
    tmp1 = src_path.parent / "a.g.ld.0.tmp.1.ll"
    tmp2 = src_path.parent / "a.g.ld.0.tmp.2.ll"
    clean_opt = "-lowerswitch -lowerinvoke -indirectbr-expand"
    transform_opt = "-mem2reg -indvars -loop-simplify -scalar-evolution"

    try:
        run_opt(src_path, tmp1, clean_opt)
        run_opt(tmp1, tmp2, "-clear-intrinsics")
        run_opt(tmp2, tmp1, transform_opt)
        run_opt(tmp1, tmp2, "-assign-ids")
        subprocess.check_output(
            f"{OPT} -load {DSE_LIB} -extract-md -out {metadata_path.as_posix()} < {tmp2.as_posix()};", 
            shell=True, stderr=subprocess.STDOUT
        )
        run_opt(tmp2, dst_path, "-strip-debug")
    except subprocess.CalledProcessError as e:
        print(f"Error processing {src_path}: {e}")
        tmp1.unlink(missing_ok=True)
        tmp2.unlink(missing_ok=True)
        dst_path.unlink(missing_ok=True)
        raise e
    finally:
        tmp1.unlink(missing_ok=True)
        tmp2.unlink(missing_ok=True)


def create_directives_tcl(directives_json: Path, output_path: Path):
    with open(directives_json, "r") as f:
        data = json.load(f)
    directives_tcl = data["HlsSolution"]["DirectiveTcl"]
    with open(output_path, "w") as f:
        directives_tcl = "\n".join(directives_tcl)
        f.write(directives_tcl)


def main(args: Dict[str, str]):
    dataset = Path(args['dataset'])
    output_folder_path = Path(args['output'])
    filtered = args['filtered']
    benchmarks = args['benchmarks']

    if benchmarks is None:
        benchmarks = [b.stem for b in list(dataset.iterdir()) if b.is_dir()]

    for benchmark in benchmarks:
        benchmark_dir = dataset / benchmark
        benchmark_out_dir = output_folder_path / benchmark
        benchmark_out_dir.mkdir(parents=True, exist_ok=True)
        solutions = [s for s in list(benchmark_dir.iterdir()) if s.is_dir()]

        for solution in solutions:
            md_json_path = solution / f"{solution.stem}_data.json"
            if not md_json_path.exists():
                print(f"Directives JSON file not found for {solution}")
                continue
            directives_tcl = solution / f"directives.tcl"
            create_directives_tcl(md_json_path, directives_tcl)

            ir_folder = solution / ("IRs" if filtered else ".autopilot/db")
            ir_path = ir_folder / "a.g.ld.0.bc"
            if not ir_path.exists():
                print(f"Intermediate representation not found for {solution}")
                continue

            solution_out_dir = benchmark_out_dir / solution.stem
            solution_out_dir.mkdir(parents=True, exist_ok=True)
            out_ir_path = ir_folder / "a.g.ld.0.mod.ll"
            out_md_path = solution_out_dir / "metadata.json"
            try:
                process_ir(ir_path, out_ir_path, out_md_path)
            except subprocess.CalledProcessError:
                print(f"Error processing {ir_path}")
                shutil.rmtree(solution_out_dir)
                continue

            metrics = extract_metrics(solution, filtered=filtered)
            with open(solution_out_dir / "metrics.json", "w") as f:
                json.dump(metrics, f, indent=2)

            build_graph(
                out_ir_path, out_md_path, 
                directive_file_path=directives_tcl,
                output_path=solution_out_dir / "graph.pt"
            )


def parse_args():
    parser = argparse.ArgumentParser(description="Process a dataset of HLS solutions.")
    parser.add_argument("-d", "--dataset", required=True,
                        help="Path to the original dataset folder")
    parser.add_argument("-o", "--output", required=True,
                        help="Path where the processed dataset should be written")
    parser.add_argument("-f", "--filtered", action="store_true",
                        help="Signal that the dataset is filtered")
    parser.add_argument("-b", "--benchmarks", nargs="+", default=None,
                        help="List of benchmarks to process")
    return vars(parser.parse_args())


if __name__ == "__main__":
    args = parse_args()
    main(args)

